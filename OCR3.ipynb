{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Set Up"
   ],
   "metadata": {
    "id": "EPWnC4AJ85Pb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZB-5LDzTjOb",
    "outputId": "dea0ed5d-70a9-4165-9135-5c7fd9b1156c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q ultralytics"
   ],
   "metadata": {
    "id": "cd7OwKHc87Uo",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "71ba9bf9-92e1-4f65-9982-6d6dcca2f078"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Configuration"
   ],
   "metadata": {
    "id": "YhklhXDK9CML"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfovnZ759Lv6",
    "outputId": "2801a042-7478-4b47-c06a-bf10ca796453"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "‚úì PyTorch version: 2.10.0+cu128\n",
      "‚úì CUDA available: True\n",
      "‚úì GPU: Tesla T4\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "CONFIG = {\n    # Data paths\n    'dataset_yaml': '/content/drive/MyDrive/dataset/data.yaml',\n    'train_images': '/content/drive/MyDrive/dataset/images/train',\n    'train_labels_yolo': '/content/drive/MyDrive/dataset/labels/train',\n    'train_labels': '/content/drive/MyDrive/dataset/labels_crnn_39/train',\n    'val_images': '/content/drive/MyDrive/dataset/images/val',\n    'val_labels_yolo': '/content/drive/MyDrive/dataset/labels/val',\n    'val_labels': '/content/drive/MyDrive/dataset/labels_crnn_39/val',\n    'test_images': '/content/drive/MyDrive/dataset/images/test',\n    'test_labels': '/content/drive/MyDrive/dataset/labels_crnn_39/test',\n\n    # Model Paths\n    'models_dir': '/content/drive/MyDrive/models',\n    'yolo_weights': '/content/drive/MyDrive/models/yolo26m.pt',\n    'crnn_weights': '/content/drive/MyDrive/models/crnn_burmese.pth',\n    'charset_path': '/content/drive/MyDrive/models/charset_39.txt',\n\n    # Training parameters\n    'yolo_epochs': 60,\n    'yolo_batch': 8,\n    'yolo_img_size': 768,\n\n    'crnn_epochs': 60,\n    'crnn_batch': 16,\n    'crnn_lr': 0.0001,\n    'ctc_blank_index': 39,\n    'seed': 42,\n    'weight_decay': 1e-5,\n    'early_stopping_patience': 12,\n\n    # CRNN architecture\n    'img_height': 32,\n    'img_width': 128,\n    'hidden_size': 64,\n    'num_classes': 39,  # numbers + alphabets + others\n}\n\nprint(\"? Configuration loaded\")\nprint(f\"  YOLO epochs: {CONFIG['yolo_epochs']}\")\nprint(f\"  CRNN epochs: {CONFIG['crnn_epochs']}\")\nprint(f\"  CRNN classes: {CONFIG['num_classes']}\")\nprint(f\"  Image size: {CONFIG['img_height']}x{CONFIG['img_width']}\")\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95qb069d9FFu",
    "outputId": "676ce9c2-c161-4df6-e6dd-b1031229ae47"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "? Configuration loaded\n",
      "  YOLO epochs: 60\n",
      "  CRNN epochs: 60\n",
      "  CRNN classes: 39\n",
      "  Image size: 32x128\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "id": "GeDDQ7S8907q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Preprocessor:\n",
    "    \"\"\"Preprocessing for NRC images and digit crops\"\"\"\n",
    "\n",
    "    def __init__(self, target_height=32, target_width=128):\n",
    "        self.target_height = target_height\n",
    "        self.target_width = target_width\n",
    "\n",
    "    def preprocess_full_image(self, image):\n",
    "        \"\"\"Preprocess full NRC image before YOLO detection\"\"\"\n",
    "        # Denoise\n",
    "        denoised = cv2.fastNlMeansDenoisingColored(image, h=10, hColor=10)\n",
    "\n",
    "        # CLAHE contrast enhancement\n",
    "        lab = cv2.cvtColor(denoised, cv2.COLOR_BGR2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "        l = clahe.apply(l)\n",
    "        enhanced = cv2.cvtColor(cv2.merge([l, a, b]), cv2.COLOR_LAB2BGR)\n",
    "\n",
    "        # Sharpen\n",
    "        kernel = np.array([[-1,-1,-1],[-1,9,-1],[-1,-1,-1]])\n",
    "        sharpened = cv2.filter2D(enhanced, -1, kernel)\n",
    "\n",
    "        return sharpened\n",
    "\n",
    "    def preprocess_for_crnn(self, image):\n",
    "      \"\"\"Preprocess concatenated digit image for CRNN input with background removal\"\"\"\n",
    "\n",
    "      # -------------------------\n",
    "      # 1. Grayscale\n",
    "      # -------------------------\n",
    "      if len(image.shape) == 3:\n",
    "          gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "      else:\n",
    "          gray = image.copy()\n",
    "\n",
    "      # -------------------------\n",
    "      # 2. Background removal (illumination correction)\n",
    "      # estimate background using large morphological closing\n",
    "      # -------------------------\n",
    "      bg_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (25, 25))\n",
    "      background = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, bg_kernel)\n",
    "\n",
    "      # subtract background\n",
    "      corrected = cv2.absdiff(gray, background)\n",
    "\n",
    "      # increase contrast\n",
    "      corrected = cv2.normalize(corrected, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "      # -------------------------\n",
    "      # 3. Denoise\n",
    "      # -------------------------\n",
    "      denoised = cv2.fastNlMeansDenoising(corrected, h=10)\n",
    "\n",
    "      # -------------------------\n",
    "      # 4. Adaptive threshold\n",
    "      # -------------------------\n",
    "      binary = cv2.adaptiveThreshold(\n",
    "          denoised,\n",
    "          255,\n",
    "          cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "          cv2.THRESH_BINARY_INV,   # digits white\n",
    "          15,\n",
    "          3\n",
    "      )\n",
    "\n",
    "      # -------------------------\n",
    "      # 5. Morphological cleaning\n",
    "      # -------------------------\n",
    "      kernel = np.ones((2, 2), np.uint8)\n",
    "      cleaned = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
    "      cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "      # -------------------------\n",
    "      # 6. Resize (preserve aspect ratio)\n",
    "      # -------------------------\n",
    "      h, w = cleaned.shape\n",
    "      scale = min(self.target_width / w, self.target_height / h)\n",
    "\n",
    "      new_w, new_h = int(w * scale), int(h * scale)\n",
    "      resized = cv2.resize(cleaned, (new_w, new_h))\n",
    "\n",
    "      canvas = np.zeros((self.target_height, self.target_width), dtype=np.uint8)\n",
    "      y_off = (self.target_height - new_h) // 2\n",
    "      x_off = (self.target_width - new_w) // 2\n",
    "      canvas[y_off:y_off+new_h, x_off:x_off+new_w] = resized\n",
    "\n",
    "      # -------------------------\n",
    "      # 7. Normalize\n",
    "      # -------------------------\n",
    "      normalized = canvas.astype(np.float32) / 255.0\n",
    "\n",
    "      return normalized\n",
    "\n",
    "print(\"‚úì Preprocessor loaded\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2r6X-QiS9yUe",
    "outputId": "7220fdb6-89e3-4016-bd5e-277e4c0d976a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úì Preprocessor loaded\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CRNN Model Architecture"
   ],
   "metadata": {
    "id": "myE-PMso96kb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class CRNN(nn.Module):\n",
    "    \"\"\"CRNN for OCR with CTC loss\"\"\"\n",
    "\n",
    "    def __init__(self, img_height=32, num_classes=10, hidden_size=256):\n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "        # CNN Feature Extractor\n",
    "        self.cnn = nn.Sequential(\n",
    "            # Conv block 1: 1 -> 64\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # 32x128 -> 16x64\n",
    "\n",
    "            # Conv block 2: 64 -> 128\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),  # 16x64 -> 8x32\n",
    "\n",
    "            # Conv block 3: 128 -> 256\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Conv block 4: 256 -> 256\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((2, 1)),  # 8x32 -> 4x32\n",
    "\n",
    "            # Conv block 5: 256 -> 512\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Conv block 6: 512 -> 512\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((2, 1)),  # 4x32 -> 2x32\n",
    "\n",
    "            # Final conv: 512 -> 512\n",
    "            nn.Conv2d(512, 512, kernel_size=2, padding=0),\n",
    "            nn.ReLU(inplace=True),  # 2x32 -> 1x31\n",
    "        )\n",
    "\n",
    "        # Bidirectional LSTM\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=512,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "\n",
    "        # Output layer: num_classes + 1 for CTC blank\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes + 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, 1, height, width)\n",
    "\n",
    "        # CNN: Extract features\n",
    "        conv = self.cnn(x)  # (batch, 512, 1, W')\n",
    "\n",
    "        # Reshape for RNN: (batch, W', 512)\n",
    "        batch, channels, height, width = conv.size()\n",
    "        assert height == 1, f\"Height should be 1, got {height}\"\n",
    "        conv = conv.squeeze(2)  # (batch, 512, W')\n",
    "        conv = conv.permute(0, 2, 1)  # (batch, W', 512)\n",
    "\n",
    "        # RNN: Sequence modeling\n",
    "        rnn_out, _ = self.rnn(conv)  # (batch, W', hidden*2)\n",
    "\n",
    "        # FC: Classification\n",
    "        output = self.fc(rnn_out)  # (batch, W', num_classes+1)\n",
    "\n",
    "        # Transpose for CTC: (W', batch, num_classes+1)\n",
    "        output = output.permute(1, 0, 2)\n",
    "\n",
    "        # Log softmax for CTC\n",
    "        output = nn.functional.log_softmax(output, dim=2)\n",
    "\n",
    "        return output\n",
    "\n",
    "print(\"‚úì CRNN model defined\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_3j0lRyd9K8l",
    "outputId": "bc98ca91-1247-4504-e533-87da771867c4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úì CRNN model defined\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Class"
   ],
   "metadata": {
    "id": "xtaeiprF9_zI"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class NRCDataset(Dataset):\n",
    "    \"\"\"Dataset for CRNN training\"\"\"\n",
    "\n",
    "    def __init__(self, image_paths, labels, preprocessor, char_to_idx):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.preprocessor = preprocessor\n",
    "        self.char_to_idx = char_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image = cv2.imread(self.image_paths[idx])\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Failed to load: {self.image_paths[idx]}\")\n",
    "\n",
    "        # Preprocess\n",
    "        processed = self.preprocessor.preprocess_for_crnn(image)\n",
    "\n",
    "        # Add channel dimension: (H, W) -> (1, H, W)\n",
    "        image_tensor = torch.FloatTensor(processed).unsqueeze(0)\n",
    "\n",
    "        # Convert label to indices\n",
    "        label_text = self.labels[idx]\n",
    "        try:\n",
    "            label_indices = [self.char_to_idx[c] for c in label_text]\n",
    "        except KeyError as e:\n",
    "            raise ValueError(\n",
    "                f\"Unknown character '{e.args[0]}' in label '{label_text}' for {self.image_paths[idx]}\"\n",
    "            )\n",
    "        label_tensor = torch.LongTensor(label_indices)\n",
    "\n",
    "        return image_tensor, label_tensor\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for variable length sequences\"\"\"\n",
    "    images, labels = zip(*batch)\n",
    "\n",
    "    # Stack images\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Get label lengths\n",
    "    label_lengths = torch.LongTensor([len(label) for label in labels])\n",
    "\n",
    "    # Concatenate labels\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    return images, labels, label_lengths\n",
    "\n",
    "print(\"? Dataset class loaded\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSqoY0UJ-Bt-",
    "outputId": "b454c61c-02c6-44e0-d7f9-1f4e90e274ca"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "? Dataset class loaded\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Functions"
   ],
   "metadata": {
    "id": "sljU0cb2-EBF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train_yolo(config):\n",
    "    \"\"\"Train YOLO detector\"\"\"\n",
    "    print(\"\" + \"=\"*60)\n",
    "    print(\"TRAINING YOLO DETECTOR\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Initialize model\n",
    "    model = YOLO('yolo26m.pt')\n",
    "\n",
    "    # Train\n",
    "    results = model.train(\n",
    "        data=config['dataset_yaml'],\n",
    "        epochs=config['yolo_epochs'],\n",
    "        imgsz=config['yolo_img_size'],\n",
    "        batch=config['yolo_batch'],\n",
    "        patience=20,\n",
    "        conf = 0.25,\n",
    "        save=True,\n",
    "        project='/content/yolo_runs',\n",
    "        name='nrc_detector',\n",
    "        exist_ok=True\n",
    "    )\n",
    "\n",
    "    # Save best weights\n",
    "    best_path = \"/content/yolo_runs/nrc_detector/weights/best.pt\"\n",
    "    shutil.copy(best_path, config['yolo_weights'])\n",
    "\n",
    "    print(f\"? YOLO training complete\")\n",
    "    print(f\"? Weights saved to: {config['yolo_weights']}\")\n",
    "\n",
    "    return YOLO(config['yolo_weights'])\n",
    "\n",
    "\n",
    "def train_crnn(train_loader, val_loader, config, device):\n",
    "    \"\"\"Train CRNN recognizer\"\"\"\n",
    "    print(\"\" + \"=\"*60)\n",
    "    print(\"TRAINING CRNN RECOGNIZER\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Initialize model\n",
    "    model = CRNN(\n",
    "        img_height=config['img_height'],\n",
    "        num_classes=config['num_classes'],\n",
    "        hidden_size=config['hidden_size']\n",
    "    ).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CTCLoss(blank=config['ctc_blank_index'], zero_infinity=True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['crnn_lr'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=10, factor=0.5\n",
    "    )\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    patience = 0\n",
    "    max_patience = 20\n",
    "\n",
    "    for epoch in range(config['crnn_epochs']):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for images, labels, label_lengths in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['crnn_epochs']}\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(images)  # (T, N, C)\n",
    "            T, N, C = outputs.size()\n",
    "\n",
    "            # Input lengths (all frames from CRNN output)\n",
    "            input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "\n",
    "            # CTC Loss\n",
    "            loss = criterion(outputs, labels, input_lengths, label_lengths)\n",
    "\n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels, label_lengths in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                T, N, C = outputs.size()\n",
    "                input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "\n",
    "                loss = criterion(outputs, labels, input_lengths, label_lengths)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}\")\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), config['crnn_weights'])\n",
    "            print(f\"? Best model saved (loss: {best_loss:.4f})\")\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if patience >= max_patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "    print(f\"? CRNN training complete\")\n",
    "    print(f\"? Best weights saved to: {config['crnn_weights']}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "print(\"? Training functions loaded\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w2HCA9_Q-GTe",
    "outputId": "c45414a5-8b40-49f9-bf03-bc3de63e31d7"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "? Training functions loaded\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference Pipeline"
   ],
   "metadata": {
    "id": "tRubsP9z-IaK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class NRCRecognizer:\n",
    "    \"\"\"End-to-end NRC recognition pipeline\"\"\"\n",
    "\n",
    "    def __init__(self, yolo_model, crnn_model, preprocessor, idx_to_char, device):\n",
    "        self.yolo = yolo_model\n",
    "        self.crnn = crnn_model.to(device)\n",
    "        self.crnn.eval()\n",
    "        self.preprocessor = preprocessor\n",
    "        self.idx_to_char = idx_to_char\n",
    "        self.device = device\n",
    "\n",
    "    def recognize(self, image_path, conf_threshold=0.25):\n",
    "        \"\"\"Recognize NRC text from image\"\"\"\n",
    "        # Load and preprocess full image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            return \"\", []\n",
    "\n",
    "        preprocessed = self.preprocessor.preprocess_full_image(image)\n",
    "\n",
    "        # YOLO detection\n",
    "        results = self.yolo(preprocessed, conf=conf_threshold, verbose=False)\n",
    "\n",
    "        boxes_xyxy = results[0].boxes.xyxy.cpu().numpy()\n",
    "        confs = results[0].boxes.conf.cpu().numpy()\n",
    "        classes = results[0].boxes.cls.cpu().numpy()\n",
    "\n",
    "        boxes = np.column_stack((boxes_xyxy, confs, classes))\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            return \"\", boxes\n",
    "\n",
    "        # Sort boxes left-to-right\n",
    "        boxes = boxes[boxes[:, 0].argsort()]\n",
    "\n",
    "        # Crop and concatenate digits\n",
    "        digit_crops = []\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2, conf, cls = box\n",
    "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "            crop = image[y1:y2, x1:x2]\n",
    "            if crop.size > 0:\n",
    "                digit_crops.append(crop)\n",
    "\n",
    "        if not digit_crops:\n",
    "            return \"\", boxes\n",
    "\n",
    "        # Concatenate digits horizontally\n",
    "        # Resize all crops to same height first\n",
    "        target_h = 32\n",
    "        resized_crops = []\n",
    "        for crop in digit_crops:\n",
    "            h, w = crop.shape[:2]\n",
    "            new_w = int(w * target_h / h)\n",
    "            resized = cv2.resize(crop, (new_w, target_h))\n",
    "            resized_crops.append(resized)\n",
    "\n",
    "        # Concatenate\n",
    "        concat_image = np.hstack(resized_crops)\n",
    "\n",
    "        # Preprocess for CRNN\n",
    "        processed = self.preprocessor.preprocess_for_crnn(concat_image)\n",
    "        image_tensor = torch.FloatTensor(processed).unsqueeze(0).unsqueeze(0)  # (1, 1, H, W)\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "\n",
    "        # CRNN recognition\n",
    "        with torch.no_grad():\n",
    "            output = self.crnn(image_tensor)  # (T, 1, C)\n",
    "\n",
    "        # Decode CTC output\n",
    "        pred_text = self._ctc_decode(output)\n",
    "\n",
    "        return pred_text, boxes\n",
    "\n",
    "    def _ctc_decode(self, output):\n",
    "        \"\"\"Decode CTC output to text\"\"\"\n",
    "        # output: (T, 1, C)\n",
    "        output = output.squeeze(1)  # (T, C)\n",
    "\n",
    "        # Get best path\n",
    "        _, preds = output.max(dim=1)  # (T,)\n",
    "        preds = preds.cpu().numpy()\n",
    "\n",
    "        # Remove blanks and duplicates\n",
    "        decoded = []\n",
    "        prev = None\n",
    "        for p in preds:\n",
    "            if p != len(self.idx_to_char) and p != prev:  # Not blank and not duplicate\n",
    "                decoded.append(self.idx_to_char[p])\n",
    "            prev = p\n",
    "\n",
    "        return ''.join(decoded)\n",
    "\n",
    "print(\"‚úì Inference pipeline loaded\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JrniNE_X-KwF",
    "outputId": "b282b695-3c9e-4f6d-b8ab-ee018fc03b54"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úì Inference pipeline loaded\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "id": "WsEZmYiA-Otu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(recognizer, test_data, verbose=True):\n",
    "    \"\"\"Evaluate the complete pipeline\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATING MODEL\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    correct = 0\n",
    "    total = len(test_data)\n",
    "    results = []\n",
    "\n",
    "    for img_path, gt_label in tqdm(test_data, desc=\"Evaluating\"):\n",
    "        pred_label, boxes = recognizer.recognize(img_path)\n",
    "\n",
    "        is_correct = (pred_label == gt_label)\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "\n",
    "        results.append({\n",
    "            'image': img_path,\n",
    "            'ground_truth': gt_label,\n",
    "            'prediction': pred_label,\n",
    "            'correct': is_correct,\n",
    "            'num_detections': len(boxes)\n",
    "        })\n",
    "\n",
    "        if verbose and not is_correct:\n",
    "            print(f\"‚úó {os.path.basename(img_path)}: GT='{gt_label}' PRED='{pred_label}'\")\n",
    "\n",
    "    accuracy = (correct / total * 100) if total > 0 else 0\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total samples: {total}\")\n",
    "    print(f\"Correct: {correct}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return accuracy, results\n",
    "\n",
    "print(\"‚úì Evaluation function loaded\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IubL-6pT-L6c",
    "outputId": "434763fd-3923-4145-90a7-e34e8c8e9076"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚úì Evaluation function loaded\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main Training Script"
   ],
   "metadata": {
    "id": "AaGscbYD-TSN"
   }
  },
  {
   "cell_type": "code",
   "source": "def save_charset(charset, path):\n    \"\"\"Save character set (one character per line).\"\"\"\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path, 'w', encoding='utf-8') as f:\n        for ch in charset:\n            f.write(ch + \"\")\n\n\ndef load_charset(path):\n    \"\"\"Load character set (one character per line).\"\"\"\n    with open(path, 'r', encoding='utf-8') as f:\n        return [line.rstrip('\\r\\n') for line in f if line.strip() != '']\n\n\ndef build_charset_from_labels(label_texts):\n    \"\"\"Build sorted unique character set from label strings.\"\"\"\n    if not label_texts:\n        return []\n    return sorted(set(''.join(label_texts)))\n\n\ndef load_data_from_labels(images_dir, labels_dir):\n    \"\"\"\n    Load data where each image has a corresponding .txt label file\n\n    Args:\n        images_dir: Directory containing images (jpg/png)\n        labels_dir: Directory containing label files (txt)\n\n    Returns:\n        List of tuples: [(image_path, label_text), ...]\n\n    Example structure:\n        images/img001.jpg\n        labels/img001.txt  (contains: full text label)\n    \"\"\"\n    data = []\n\n    # Get all images\n    image_files = glob.glob(os.path.join(images_dir, '*.jpg'))+glob.glob(os.path.join(images_dir, '*.png')) +glob.glob(os.path.join(images_dir, '*.jpeg'))\n\n    print(f\"?? Loading from {images_dir}\")\n    print(f\"   Found {len(image_files)} images\")\n\n    loaded_count = 0\n    skipped_count = 0\n\n    for img_path in image_files:\n        # Get corresponding label file\n        base_name = os.path.splitext(os.path.basename(img_path))[0]\n        label_path = os.path.join(labels_dir, base_name + '.txt')\n\n        if not os.path.exists(label_path):\n            print(f\"??  Missing label: {base_name}.txt\")\n            skipped_count += 1\n            continue\n\n        # Read label text\n        try:\n            with open(label_path, 'r', encoding='utf-8') as f:\n                label_text = f.read().strip()\n\n            if label_text:\n                data.append((img_path, label_text))\n                loaded_count += 1\n            else:\n                print(f\"??  Empty label in {base_name}.txt\")\n                skipped_count += 1\n        except Exception as e:\n            print(f\"??  Error reading {base_name}.txt: {e}\")\n            skipped_count += 1\n\n    print(f\"? Loaded: {loaded_count} samples\")\n    if skipped_count > 0:\n        print(f\"??  Skipped: {skipped_count} samples\")\n\n    return data\n\nprint(\"? Data loading function ready\")\n",
   "metadata": {
    "id": "TLBMoLib-isp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "68d06210-97f0-410b-d9e0-3b3f7974d71e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "? Data loading function ready\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "def main_train():\n    \"\"\"\n    Complete training pipeline for CRNN\n\n    This function:\n    1. Loads your existing YOLO11m model\n    2. Loads training and validation data\n    3. Trains the CRNN model from scratch\n    4. Saves the trained CRNN model\n    \"\"\"\n\n    print(\"\" + \"=\"*70)\n    print(\"BURMESE NRC OCR - TRAINING PIPELINE\")\n    print(\"=\"*70)\n\n    # Check device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"???  Device: {device}\")\n    if device.type == 'cuda':\n        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n\n    # Initialize preprocessor\n    preprocessor = Preprocessor(\n        target_height=CONFIG['img_height'],\n        target_width=CONFIG['img_width']\n    )\n    print(f\"?? Preprocessor initialized\")\n    print(f\"   Target size: {CONFIG['img_height']}x{CONFIG['img_width']}\")\n\n    # ========== LOAD YOLO MODEL ==========\n    print(\"\" + \"=\"*70)\n    print(\"STEP 1: LOADING/CREATING YOLO MODEL\")\n    print(\"=\"*70)\n\n    yolo_path = CONFIG['yolo_weights']\n    best_path = '/content/yolo_runs/nrc_detector/weights/best.pt'\n\n    if os.path.exists(yolo_path):\n        print(f\"?? Using existing YOLO weights: {yolo_path}\")\n        yolo_model = YOLO(yolo_path)\n    elif os.path.exists(best_path):\n        print(f\"?? Using latest YOLO run weights: {best_path}\")\n        yolo_model = YOLO(best_path)\n    else:\n        print(\"\" + \"=\"*70)\n        print(\"STEP 1: TRAINING YOLO MODEL\")\n        print(\"=\"*70)\n\n        yolo_model = train_yolo(CONFIG)\n\n    # ========== LOAD DATA ==========\n    print(f\"\" + \"=\"*70)\n    print(\"STEP 2: LOADING TRAINING DATA\")\n    print(\"=\"*70)\n\n    try:\n        train_data = load_data_from_labels(CONFIG['train_images'], CONFIG['train_labels'])\n        val_data = load_data_from_labels(CONFIG['val_images'], CONFIG['val_labels'])\n    except Exception as e:\n        print(f\"? ERROR loading data: {e}\")\n        print(f\"?? Please check:\")\n        print(f\"   1. Paths in CONFIG are correct\")\n        print(f\"   2. Image and label files exist\")\n        print(f\"   3. Label files contain valid characters\")\n        return None, None\n\n    if len(train_data) == 0:\n        print(f\"? ERROR: No training data found!\")\n        print(f\"   Train images path: {CONFIG['train_images']}\")\n        print(f\"   Train labels path: {CONFIG['train_labels']}\")\n        return None, None\n\n    if len(val_data) == 0:\n        print(f\"??  WARNING: No validation data found!\")\n        print(f\"   Using 20% of training data for validation...\")\n        # Split training data\n        split_idx = int(len(train_data) * 0.8)\n        val_data = train_data[split_idx:]\n        train_data = train_data[:split_idx]\n\n    print(f\"?? Data summary:\")\n    print(f\"   Training samples: {len(train_data)}\")\n    print(f\"   Validation samples: {len(val_data)}\")\n\n    # Show some examples\n    print(f\"?? Sample data (first 3):\")\n    for i, (img_path, label) in enumerate(train_data[:3]):\n        print(f\"   {i+1}. {os.path.basename(img_path)} ? '{label}'\")\n\n    # Separate image paths and labels\n    train_images = [item[0] for item in train_data]\n    train_labels = [item[1] for item in train_data]\n    val_images = [item[0] for item in val_data]\n    val_labels = [item[1] for item in val_data]\n\n    # ========== CHARACTER SET ==========\n    print(f\"\" + \"=\"*70)\n    print(\"STEP 2B: BUILDING CHARACTER SET\")\n    print(\"=\"*70)\n\n    charset = load_charset(CONFIG['charset_path'])\n    if not charset:\n        print(\"? ERROR: No characters found in labels!\")\n        return yolo_model, None\n\n    print(f\"?? Loaded charset: {len(charset)} classes\")\n    print(f\"   {''.join(charset)}\")\n\n    if len(charset) != CONFIG['num_classes']:\n        print(f\"??  CONFIG['num_classes'] ({CONFIG['num_classes']}) does not match charset size ({len(charset)}).\")\n        print(\"   Please fix CONFIG['num_classes'] to match the charset.\")\n        return None, None\n\n    CONFIG['ctc_blank_index'] = CONFIG['num_classes']\n\n    char_to_idx = {char: idx for idx, char in enumerate(charset)}\n    idx_to_char = {idx: char for idx, char in enumerate(charset)}\n\n    print(f\"?? Character set ready\")\n    print(f\"   Total classes: {len(charset)}\")\n\n    # ========== CREATE DATASETS ==========\n    print(f\"\" + \"=\"*70)\n    print(\"STEP 3: CREATING PYTORCH DATASETS\")\n    print(\"=\"*70)\n\n    train_dataset = NRCDataset(train_images, train_labels, preprocessor, char_to_idx)\n    val_dataset = NRCDataset(val_images, val_labels, preprocessor, char_to_idx)\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=CONFIG['crnn_batch'],\n        shuffle=True,\n        collate_fn=collate_fn,\n        num_workers=2,\n        pin_memory=True if device.type == 'cuda' else False\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=CONFIG['crnn_batch'],\n        shuffle=False,\n        collate_fn=collate_fn,\n        num_workers=2,\n        pin_memory=True if device.type == 'cuda' else False\n    )\n\n    print(f\"? Datasets created\")\n    print(f\"   Train batches: {len(train_loader)}\")\n    print(f\"   Val batches: {len(val_loader)}\")\n    print(f\"   Batch size: {CONFIG['crnn_batch']}\")\n\n    # ========== TRAIN CRNN ==========\n    print(f\"\" + \"=\"*70)\n    print(\"STEP 4: TRAINING CRNN MODEL\")\n    print(\"=\"*70)\n\n    print(f\"??  Training configuration:\")\n    print(f\"   Epochs: {CONFIG['crnn_epochs']}\")\n    print(f\"   Learning rate: {CONFIG['crnn_lr']}\")\n    print(f\"   Hidden size: {CONFIG['hidden_size']}\")\n    print(f\"   Classes: {CONFIG['num_classes']}\")\n    print(f\"   Image size: {CONFIG['img_height']}x{CONFIG['img_width']}\")\n\n    print(f\"?? Starting training...\")\n    print(f\"   This may take 30 min - 2 hours depending on dataset size\")\n    print(f\"   Progress will be shown below\")\n\n    try:\n        crnn_model = train_crnn(train_loader, val_loader, CONFIG, device)\n    except Exception as e:\n        print(f\"? ERROR during training: {e}\")\n        import traceback\n        traceback.print_exc()\n        return yolo_model, None\n\n    print(f\"\" + \"=\"*70)\n    print(\"? TRAINING COMPLETE!\")\n    print(\"=\"*70)\n    print(f\"? YOLO model: {CONFIG['yolo_weights']}\")\n    print(f\"? CRNN model: {CONFIG['crnn_weights']}\")\n    print(f\"? Charset file: {CONFIG['charset_path']}\")\n    print(f\"?? Next steps:\")\n    print(f\"   1. Run Cell 11 to test on a single image\")\n    print(f\"   2. Run Cell 9 to evaluate on test set\")\n\n    return yolo_model, crnn_model\n",
   "metadata": {
    "id": "P_WIUK4r-XWS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "yolo_model, crnn_model = main_train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ChNCJ_xfhXmA",
    "outputId": "b6334b8a-d83e-4cff-9789-95399eaaeb16"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "BURMESE NRC OCR - TRAINING PIPELINE\n",
      "======================================================================\n",
      "???  Device: cuda\n",
      "   GPU: Tesla T4\n",
      "   Memory: 15.6 GB\n",
      "?? Preprocessor initialized\n",
      "   Target size: 32x128\n",
      "======================================================================\n",
      "STEP 1: LOADING/CREATING YOLO MODEL\n",
      "======================================================================\n",
      "======================================================================\n",
      "STEP 1: TRAINING YOLO MODEL\n",
      "======================================================================\n",
      "============================================================\n",
      "TRAINING YOLO DETECTOR\n",
      "============================================================\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m.pt to 'yolo26m.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 42.2MB 163.9MB/s 0.3s\n",
      "Ultralytics 8.4.17 üöÄ Python-3.12.12 torch-2.10.0+cu128 CUDA:0 (Tesla T4, 14913MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=0.25, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/dataset/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=60, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=768, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo26m.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=nrc_detector, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/yolo_runs, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/yolo_runs/nrc_detector, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\u001b[KDownloading https://ultralytics.com/assets/Arial.Unicode.ttf to '/root/.config/Ultralytics/Arial.Unicode.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 22.2MB 271.2MB/s 0.1s\n",
      "Overriding model.yaml nc=80 with nc=39\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5, 3, True]        \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1974784  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True, 0.5, True]\n",
      " 23        [16, 19, 22]  1   2858754  ultralytics.nn.modules.head.Detect           [39, 1, True, [256, 512, 512]]\n",
      "YOLO26m summary: 280 layers, 21,833,026 parameters, 21,833,026 gradients, 75.0 GFLOPs\n",
      "\n",
      "Transferred 756/768 items from pretrained weights\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n.pt to 'yolo26n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.3MB 103.7MB/s 0.1s\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.4¬±0.1 ms, read: 0.8¬±0.7 MB/s, size: 367.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/dataset/labels/train.cache... 493 images, 11 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 504/504 96.1Mit/s 0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/dataset/images/train/IMG_054565.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/content/drive/MyDrive/dataset/images/train/IMG_12242446.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.9¬±0.5 ms, read: 3.1¬±4.4 MB/s, size: 1154.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/dataset/labels/val.cache... 93 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 93/93 7.4Mit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/dataset/images/val/IMG_12195308.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/content/drive/MyDrive/dataset/images/val/IMG_12249470.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000233, momentum=0.9) with parameter groups 124 weight(decay=0.0), 136 weight(decay=0.0005), 136 bias(decay=0.0)\n",
      "Plotting labels to /content/yolo_runs/nrc_detector/labels.jpg... \n",
      "Image sizes 768 train, 768 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1m/content/yolo_runs/nrc_detector\u001b[0m\n",
      "Starting training for 60 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 22.6MB/s 0.0s\n",
      "\u001b[K       1/60       6.6G       1.84      5.249   0.005857        161        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.5it/s 41.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 2.7s/it 16.1s\n",
      "                   all         93       1100     0.0473    0.00849     0.0277     0.0132\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/60       6.8G      1.637      3.674   0.005137        199        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 2.0it/s 31.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.8it/s 1.6s\n",
      "                   all         93       1100      0.126     0.0252     0.0764     0.0468\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/60      6.75G      1.612      2.792   0.005071        142        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 2.0it/s 31.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.2it/s 1.4s\n",
      "                   all         93       1100      0.229     0.0865       0.16     0.0991\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/60       6.8G      1.565        2.2   0.004652        172        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 2.0it/s 31.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.1it/s 1.5s\n",
      "                   all         93       1100      0.259      0.152      0.206      0.121\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/60      6.74G      1.525      1.822   0.004617        153        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 32.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.3it/s 1.4s\n",
      "                   all         93       1100      0.332      0.204      0.275      0.164\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/60      6.81G      1.506      1.551   0.004548         99        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.3it/s 1.4s\n",
      "                   all         93       1100      0.346      0.278      0.317      0.189\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/60      6.82G      1.487      1.367   0.004523        146        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.1it/s 1.9s\n",
      "                   all         93       1100      0.345      0.314      0.339      0.203\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/60      6.78G      1.468      1.228   0.004449        125        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.6it/s 1.7s\n",
      "                   all         93       1100      0.415      0.338      0.388      0.236\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/60       6.8G      1.455      1.159   0.004424        194        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.2it/s 1.4s\n",
      "                   all         93       1100      0.417      0.346      0.394      0.242\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/60      6.81G      1.417      1.072   0.004237        107        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.3it/s 1.4s\n",
      "                   all         93       1100       0.46      0.374      0.418      0.258\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/60      6.79G      1.385     0.9711    0.00419        150        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.0it/s 2.0s\n",
      "                   all         93       1100       0.48      0.362      0.429      0.263\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/60      6.77G      1.386     0.9605   0.004186        167        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.3it/s 1.4s\n",
      "                   all         93       1100      0.527      0.399      0.479       0.29\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/60      6.75G      1.382      0.932   0.004043        176        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.1it/s 1.9s\n",
      "                   all         93       1100      0.498      0.384      0.452      0.273\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/60       6.8G      1.346     0.8637   0.003887        255        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.2it/s 1.4s\n",
      "                   all         93       1100      0.531       0.43      0.485      0.288\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/60      6.78G      1.294     0.8396   0.003802        172        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.3it/s 1.4s\n",
      "                   all         93       1100      0.498      0.454      0.484      0.286\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/60      6.81G      1.286     0.8113   0.003928        139        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.0it/s 1.5s\n",
      "                   all         93       1100       0.52      0.433      0.487      0.287\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/60       6.8G      1.283     0.7803   0.003727        238        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.3it/s 1.4s\n",
      "                   all         93       1100      0.567      0.446      0.503      0.295\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      18/60      6.79G      1.244     0.7537   0.003612        185        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.2it/s 1.9s\n",
      "                   all         93       1100      0.562      0.443      0.511      0.308\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      19/60      6.81G       1.25     0.7166   0.003642        144        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.2it/s 1.4s\n",
      "                   all         93       1100      0.561      0.442      0.517       0.31\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      20/60      6.81G      1.191     0.7012    0.00348        170        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.9it/s 1.5s\n",
      "                   all         93       1100      0.574      0.444      0.522      0.309\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      21/60      6.72G      1.188     0.6703   0.003449        207        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.2it/s 1.4s\n",
      "                   all         93       1100      0.572      0.458      0.528      0.313\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      22/60      6.78G      1.178     0.6549    0.00333        196        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.0it/s 2.0s\n",
      "                   all         93       1100      0.574      0.441      0.521      0.301\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      23/60      6.79G      1.136     0.6397   0.003227        147        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.2it/s 1.4s\n",
      "                   all         93       1100       0.54      0.482      0.533      0.313\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      24/60      6.79G      1.116     0.6188   0.003168        187        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 32.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.9it/s 1.6s\n",
      "                   all         93       1100      0.572      0.463       0.53      0.314\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      25/60      6.81G      1.136     0.6256   0.003288        193        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.2it/s 1.4s\n",
      "                   all         93       1100      0.595      0.474      0.548      0.322\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      26/60      6.78G      1.097     0.5885   0.003215        185        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.1it/s 1.5s\n",
      "                   all         93       1100      0.604      0.458      0.542      0.314\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      27/60      6.81G      1.095     0.5983   0.003014        185        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.2it/s 1.9s\n",
      "                   all         93       1100      0.627      0.467      0.545       0.32\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      28/60      6.79G      1.066     0.5877   0.003027        130        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.1it/s 1.5s\n",
      "                   all         93       1100      0.614      0.521      0.578      0.342\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      29/60      6.76G      1.047     0.5633   0.002965        143        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 32.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.3it/s 1.8s\n",
      "                   all         93       1100      0.632      0.484      0.569      0.334\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      30/60       6.8G      1.043     0.5526   0.002894        186        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.1it/s 1.5s\n",
      "                   all         93       1100      0.584      0.486      0.559      0.329\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      31/60      6.77G       1.05     0.5706   0.003021        204        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.4it/s 1.7s\n",
      "                   all         93       1100      0.611      0.498      0.565      0.326\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      32/60      6.81G      1.024     0.5556   0.002846         86        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.3it/s 1.4s\n",
      "                   all         93       1100      0.625      0.487      0.562      0.339\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      33/60      6.79G     0.9869     0.5245   0.002688        116        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.8it/s 1.6s\n",
      "                   all         93       1100      0.622      0.493      0.565       0.33\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      34/60      6.78G     0.9683     0.5088   0.002696        207        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.1it/s 1.5s\n",
      "                   all         93       1100      0.616      0.493      0.567      0.333\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      35/60      6.81G     0.9608     0.5109   0.002708        177        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 32.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.1it/s 1.5s\n",
      "                   all         93       1100       0.62      0.493      0.569      0.337\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      36/60      6.79G     0.9444     0.5015   0.002602        161        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.2it/s 1.4s\n",
      "                   all         93       1100      0.637       0.51      0.585      0.348\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      37/60      6.74G     0.9248     0.4865   0.002558        234        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.2it/s 1.4s\n",
      "                   all         93       1100      0.641      0.512       0.58      0.344\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      38/60      6.78G     0.9368        0.5   0.002551        205        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.2it/s 1.9s\n",
      "                   all         93       1100      0.631       0.49      0.571       0.34\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      39/60       6.8G      0.918      0.492   0.002496        177        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 32.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.1it/s 1.5s\n",
      "                   all         93       1100      0.634      0.512      0.585      0.349\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      40/60       6.8G     0.9224     0.4965   0.002501        172        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.0it/s 2.0s\n",
      "                   all         93       1100      0.629      0.498      0.576      0.346\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      41/60      6.79G     0.8867     0.4593   0.002458        148        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.9it/s 1.5s\n",
      "                   all         93       1100      0.663      0.518      0.587      0.346\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      42/60      6.79G     0.8813     0.4664   0.002385        193        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 2.9it/s 2.0s\n",
      "                   all         93       1100      0.631      0.496      0.578      0.341\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      43/60      6.79G     0.8839     0.4648   0.002393        173        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.0it/s 1.5s\n",
      "                   all         93       1100      0.656      0.493      0.574      0.336\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      44/60      6.78G     0.8606     0.4576   0.002332        181        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.2it/s 1.9s\n",
      "                   all         93       1100      0.625      0.497      0.571      0.332\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      45/60      6.72G     0.8387     0.4357   0.002277        131        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.2it/s 1.4s\n",
      "                   all         93       1100      0.619      0.501      0.573      0.338\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      46/60       6.8G     0.8395     0.4356   0.002324        215        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 32.9s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.0it/s 2.0s\n",
      "                   all         93       1100      0.583      0.483      0.548      0.322\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      47/60      6.82G     0.8674       0.47   0.002291        172        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.1it/s 1.5s\n",
      "                   all         93       1100      0.633      0.501      0.578      0.339\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      48/60      6.78G     0.8481      0.433   0.002191        195        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.2it/s 1.9s\n",
      "                   all         93       1100       0.64      0.487       0.57      0.334\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      49/60       6.8G      0.809     0.4242   0.002228        206        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.1it/s 1.5s\n",
      "                   all         93       1100      0.612      0.503      0.576      0.338\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      50/60      6.77G     0.8144     0.4272   0.002254        161        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 33.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.0it/s 1.5s\n",
      "                   all         93       1100      0.606      0.507      0.575      0.338\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      51/60      6.78G     0.8315     0.4071   0.002449         83        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 34.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.2it/s 1.4s\n",
      "                   all         93       1100      0.581      0.486      0.546      0.322\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      52/60      6.78G     0.8183     0.3817    0.00234         88        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 32.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.2it/s 1.4s\n",
      "                   all         93       1100      0.605      0.487      0.562      0.329\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      53/60      6.73G     0.8229     0.3754   0.002362         81        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 32.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.3it/s 1.8s\n",
      "                   all         93       1100      0.626      0.488      0.566      0.336\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      54/60      6.78G     0.7981     0.3658   0.002228         85        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 32.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.1it/s 1.4s\n",
      "                   all         93       1100      0.596      0.473      0.545      0.317\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      55/60      6.78G     0.7719     0.3667   0.002187         94        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 32.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 2.8it/s 2.1s\n",
      "                   all         93       1100      0.597       0.48      0.548      0.321\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      56/60      6.79G     0.7935     0.3649   0.002231         90        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 32.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.1it/s 1.5s\n",
      "                   all         93       1100      0.633      0.488      0.557      0.331\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      57/60      6.78G     0.7779     0.3604   0.002207         79        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 32.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 3.1it/s 1.9s\n",
      "                   all         93       1100      0.632      0.493       0.57      0.341\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      58/60      6.78G     0.7642       0.34   0.002167         75        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 32.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.2it/s 1.4s\n",
      "                   all         93       1100      0.628      0.494       0.57       0.34\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      59/60      6.78G     0.7811     0.3605   0.002207         79        768: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63/63 1.9it/s 32.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 4.2it/s 1.4s\n",
      "                   all         93       1100      0.629      0.491      0.569       0.34\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 20 epochs. Best results observed at epoch 39, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=20) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "59 epochs completed in 0.600 hours.\n",
      "Optimizer stripped from /content/yolo_runs/nrc_detector/weights/last.pt, 44.2MB\n",
      "Optimizer stripped from /content/yolo_runs/nrc_detector/weights/best.pt, 44.2MB\n",
      "\n",
      "Validating /content/yolo_runs/nrc_detector/weights/best.pt...\n",
      "Ultralytics 8.4.17 üöÄ Python-3.12.12 torch-2.10.0+cu128 CUDA:0 (Tesla T4, 14913MiB)\n",
      "YOLO26m summary (fused): 132 layers, 20,379,521 parameters, 0 gradients, 68.0 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6/6 2.7it/s 2.2s\n",
      "                   all         93       1100      0.633      0.511      0.585      0.349\n",
      "               digit_0         79        123       0.93      0.976      0.983      0.543\n",
      "               digit_1         81        131      0.943      0.885       0.93      0.487\n",
      "               digit_2         68         97       0.91      0.938      0.952      0.478\n",
      "               digit_3         38         49      0.851      0.816      0.872      0.491\n",
      "               digit_4         47         60      0.817      0.817      0.864      0.465\n",
      "               digit_5         38         45      0.729      0.778      0.797      0.496\n",
      "               digit_6         35         46      0.857      0.913      0.944      0.548\n",
      "               digit_7         36         45      0.933      0.933      0.963      0.577\n",
      "               digit_8         44         55      0.943      0.909      0.942      0.545\n",
      "               digit_9         52         63      0.718       0.81      0.819      0.443\n",
      "                 ·Äî·Ä≠·ÄØ·ÄÑ·Ä∫         91         91      0.957      0.978      0.987      0.648\n",
      "                     ·ÄÄ         30         35      0.829      0.829      0.876      0.559\n",
      "                     ·ÄÅ          9          9          1      0.222      0.611      0.361\n",
      "                     ·ÄÇ          7          7      0.714      0.714      0.612      0.386\n",
      "                     ·ÄÉ          3          3          0          0          0          0\n",
      "                     ·ÄÑ          1          1          0          0          0          0\n",
      "                     ·ÄÖ          6          6          1      0.167      0.583      0.467\n",
      "                     ·ÄÜ          1          1          0          0          0          0\n",
      "                     ·Äá          5          5          0          0          0          0\n",
      "                     ·Ää          1          1          0          0          0          0\n",
      "                     ·Äè          1          1          0          0          0          0\n",
      "                     ·Äê         23         24        0.7      0.583       0.65      0.341\n",
      "                     ·Äë          3          3          0          0          0          0\n",
      "                     ·Äí         13         14      0.846      0.786      0.767      0.489\n",
      "                     ·Äì          1          1          0          0          0          0\n",
      "                     ·Äî         46         49      0.936      0.898      0.931      0.515\n",
      "                     ·Äï         17         17      0.667      0.471      0.547      0.319\n",
      "                     ·Äñ          3          3          0          0          0          0\n",
      "                     ·Äó          3          3          1      0.667      0.833      0.467\n",
      "                     ·Äò          3          3      0.667      0.667      0.777      0.544\n",
      "                     ·Äô         35         38      0.794      0.711      0.795      0.457\n",
      "                     ·Äö          1          1          0          0          0          0\n",
      "                     ·Äõ         17         17      0.882      0.882      0.892      0.648\n",
      "                     ·Äú         15         15      0.643        0.6      0.686      0.436\n",
      "                     ·Äù          3          3          1      0.667      0.833        0.5\n",
      "                     ·Äû         13         13      0.438      0.538      0.484      0.284\n",
      "                     ·Äü          5          5          1        0.2        0.6       0.42\n",
      "                     ·Ä°          9          9          1      0.333      0.667      0.353\n",
      "                     ·Ä•          8          8          1       0.25      0.625      0.347\n",
      "Speed: 0.3ms preprocess, 9.1ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/content/yolo_runs/nrc_detector\u001b[0m\n",
      "? YOLO training complete\n",
      "? Weights saved to: /content/drive/MyDrive/models/yolo26m.pt\n",
      "======================================================================\n",
      "STEP 2: LOADING TRAINING DATA\n",
      "======================================================================\n",
      "?? Loading from /content/drive/MyDrive/dataset/images/train\n",
      "   Found 500 images\n",
      "??  Missing label: img_234093.txt\n",
      "??  Missing label: all.txt\n",
      "??  Missing label: all_r.txt\n",
      "??  Missing label: all_r_g.txt\n",
      "??  Missing label: all_o.txt\n",
      "??  Empty label in types.txt\n",
      "??  Missing label: IMG_071460.txt\n",
      "??  Missing label: img_101965.txt\n",
      "? Loaded: 492 samples\n",
      "??  Skipped: 8 samples\n",
      "?? Loading from /content/drive/MyDrive/dataset/images/val\n",
      "   Found 93 images\n",
      "? Loaded: 93 samples\n",
      "?? Data summary:\n",
      "   Training samples: 492\n",
      "   Validation samples: 93\n",
      "?? Sample data (first 3):\n",
      "   1. IMG_085680.jpg ? '·ÅÅ·ÅÇ/·ÄÄ·Äô·Äê(·Äî·Ä≠·ÄØ·ÄÑ·Ä∫)·ÅÄ·Åà·ÅÖ·ÅÜ·Åà·ÅÄ'\n",
      "   2. IMG_094048.jpg ? '·ÅÅ·ÅÄ/·Äõ·Äô·Äî(·Äî·Ä≠·ÄØ·ÄÑ·Ä∫)·Åâ·Åâ·ÅÑ·ÅÄ·ÅÑ·Åà'\n",
      "   3. IMG_060908.jpg ? '·ÅÅ·ÅÇ/·Äí·Äï·Äî(·Äî·Ä≠·ÄØ·ÄÑ·Ä∫)·ÅÄ·ÅÜ·ÅÄ·Åâ·ÅÄ·Åà'\n",
      "======================================================================\n",
      "STEP 2B: BUILDING CHARACTER SET\n",
      "======================================================================\n",
      "?? Unique characters found: 55\n",
      "   \n",
      " ()./0123456789·ÄÄ·ÄÅ·ÄÇ·ÄÉ·ÄÑ·ÄÖ·ÄÜ·Äá·Ää·Äê·Äë·Äí·Äî·Äï·Äñ·Äó·Äò·Äô·Äö·Äõ·Äú·Äù·Äû·Äü·Ä°·Ä•·Ä≠·ÄØ·Ä∫·ÅÄ·ÅÅ·ÅÇ·ÅÉ·ÅÑ·ÅÖ·ÅÜ·Åá·Åà·Åâ\n",
      "??  CONFIG['num_classes'] (39) does not match charset size (55).\n",
      "   Updating num_classes and ctc_blank_index to match the charset.\n",
      "? Charset saved to: /content/drive/MyDrive/models/charset.txt\n",
      "?? Character set ready\n",
      "   Total classes: 55\n",
      "======================================================================\n",
      "STEP 3: CREATING PYTORCH DATASETS\n",
      "======================================================================\n",
      "? Datasets created\n",
      "   Train batches: 31\n",
      "   Val batches: 6\n",
      "   Batch size: 16\n",
      "======================================================================\n",
      "STEP 4: TRAINING CRNN MODEL\n",
      "======================================================================\n",
      "??  Training configuration:\n",
      "   Epochs: 60\n",
      "   Learning rate: 0.0001\n",
      "   Hidden size: 64\n",
      "   Classes: 55\n",
      "   Image size: 32x128\n",
      "?? Starting training...\n",
      "   This may take 30 min - 2 hours depending on dataset size\n",
      "   Progress will be shown below\n",
      "============================================================\n",
      "TRAINING CRNN RECOGNIZER\n",
      "============================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 1/60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [07:23<00:00, 14.32s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1: Train Loss=1.2122, Val Loss=4.5805\n",
      "? Best model saved (loss: 4.5805)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 2/60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [07:23<00:00, 14.31s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 2: Train Loss=1.0237, Val Loss=3.7900\n",
      "? Best model saved (loss: 3.7900)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 3/60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [07:27<00:00, 14.44s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 3: Train Loss=0.9030, Val Loss=3.5145\n",
      "? Best model saved (loss: 3.5145)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 4/60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [07:24<00:00, 14.33s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 4: Train Loss=0.8580, Val Loss=3.4162\n",
      "? Best model saved (loss: 3.4162)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 5/60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [07:20<00:00, 14.21s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 5: Train Loss=0.8323, Val Loss=3.3551\n",
      "? Best model saved (loss: 3.3551)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 6/60: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [07:18<00:00, 14.14s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 6: Train Loss=0.8181, Val Loss=3.2941\n",
      "? Best model saved (loss: 3.2941)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 7/60:   3%|‚ñé         | 1/31 [00:21<10:30, 21.01s/it]"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference Example"
   ],
   "metadata": {
    "id": "EHsaNK6c-aZM"
   }
  },
  {
   "cell_type": "code",
   "source": "def test_inference(image_path):\n    \"\"\"Test inference on a single image\"\"\"\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # Character mappings\n    try:\n        charset = load_charset(CONFIG['charset_path'])\n        if not charset:\n            raise ValueError(\"Charset file is empty\")\n    except Exception as e:\n        print(f\"??  Could not load charset: {e}\")\n        print(\"   Please fix CONFIG['charset_path'] to point to charset_39.txt\")\n        return None, None\n\n    idx_to_char = {idx: char for idx, char in enumerate(charset)}\n\n    # Load models\n    yolo_model = YOLO(CONFIG['yolo_weights'])\n\n    crnn_model = CRNN(\n        img_height=CONFIG['img_height'],\n        num_classes=len(charset),\n        hidden_size=CONFIG['hidden_size']\n    )\n    crnn_model.load_state_dict(torch.load(CONFIG['crnn_weights']))\n    crnn_model = crnn_model.to(device)\n\n    # Initialize preprocessor and recognizer\n    preprocessor = Preprocessor(\n        target_height=CONFIG['img_height'],\n        target_width=CONFIG['img_width']\n    )\n\n    recognizer = NRCRecognizer(yolo_model, crnn_model, preprocessor, idx_to_char, device)\n\n    # Recognize\n    result, boxes = recognizer.recognize(image_path)\n\n    print(f\"\n{'='*60}\")\n    print(f\"Image: {os.path.basename(image_path)}\")\n    print(f\"Detected boxes: {len(boxes)}\")\n    print(f\"Recognized: {result}\")\n    print(f\"{'='*60}\")\n\n    # Visualize\n    img = cv2.imread(image_path)\n    for box in boxes:\n        x1, y1, x2, y2, conf, cls = box\n        x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n    plt.figure(figsize=(12, 6))\n    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    plt.title(f\"Recognized: {result}\")\n    plt.axis('off')\n    plt.show()\n\n    return result, boxes\n\n# Example usage:\n#test_inference('/content/drive/MyDrive/dataset/images/val/IMG_052905.jpg')\n#test_inference('/content/drive/MyDrive/dataset/images/val/img_072309.jpeg')\n#test_inference('/content/drive/MyDrive/dataset/images/train/IMG_110349.jpeg')\n",
   "metadata": {
    "id": "QRRw20Cj-da1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_inference('/content/drive/MyDrive/dataset/images/val/img_072309.jpeg')"
   ],
   "metadata": {
    "id": "40VewL9Y5Sg5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_inference('/content/drive/MyDrive/dataset/images/val/IMG_052905.jpg')"
   ],
   "metadata": {
    "id": "-2wCxBS_5XOa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def test_yolo_on_multiple_images(image_folder, num_images=5):\n",
    "    \"\"\"\n",
    "    Test YOLO on multiple images from a folder\n",
    "    \"\"\"\n",
    "    # Get all images in folder\n",
    "    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']\n",
    "    image_paths = []\n",
    "\n",
    "    for ext in image_extensions:\n",
    "        image_paths.extend(glob.glob(os.path.join(image_folder, ext)))\n",
    "\n",
    "    if not image_paths:\n",
    "        print(f\"‚ùå No images found in {image_folder}\")\n",
    "        return\n",
    "\n",
    "    # Limit to num_images\n",
    "    test_images = image_paths[:num_images]\n",
    "\n",
    "    print(f\"üß™ Testing YOLO on {len(test_images)} images...\")\n",
    "    print(f\"   Folder: {image_folder}\")\n",
    "\n",
    "    # Load YOLO model once\n",
    "    try:\n",
    "        model = YOLO('yolo26n.pt')\n",
    "        print(f\"‚úì Loaded YOLOv8n model\")\n",
    "    except:\n",
    "        print(\"‚ùå Could not load YOLO model\")\n",
    "        return\n",
    "\n",
    "    results_summary = []\n",
    "\n",
    "    for img_path in test_images:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing: {os.path.basename(img_path)}\")\n",
    "\n",
    "        # Test detection\n",
    "        detections = test_yolo_detection(img_path, model=model)\n",
    "\n",
    "        if detections is not None:\n",
    "            boxes, confidences, classes = detections\n",
    "            results_summary.append({\n",
    "                'image': os.path.basename(img_path),\n",
    "                'detections': len(boxes),\n",
    "                'avg_confidence': np.mean(confidences) if len(confidences) > 0 else 0\n",
    "            })\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üìä TEST SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    if results_summary:\n",
    "        total_detections = sum(r['detections'] for r in results_summary)\n",
    "        avg_detections = total_detections / len(results_summary)\n",
    "        avg_confidence = np.mean([r['avg_confidence'] for r in results_summary])\n",
    "\n",
    "        print(f\"Images tested: {len(results_summary)}\")\n",
    "        print(f\"Total detections: {total_detections}\")\n",
    "        print(f\"Average detections per image: {avg_detections:.1f}\")\n",
    "        print(f\"Average confidence: {avg_confidence:.3f}\")\n",
    "\n",
    "        print(f\"\\nDetails:\")\n",
    "        for result in results_summary:\n",
    "            print(f\"  {result['image']}: {result['detections']} detections \"\n",
    "                  f\"(avg conf: {result['avg_confidence']:.3f})\")\n",
    "    else:\n",
    "        print(\"‚ùå No detections in any image!\")"
   ],
   "metadata": {
    "id": "SN-xBFJdCEEH"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}